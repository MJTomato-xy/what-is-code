<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Let's Begin</title>
    <link href="chapter2.css" rel="stylesheet" type="text/css">
</head>
<body>
<div class="header">
 <header>
     <a id="home" href="https://mjtomato-xy.github.io/project4/">Back</a>
     <h1>Let's Begin</h1>
 </header>   
</div>
<div class="row">
    <div class="column side">
        <ul>
        <li><a href="chapter22.html">2.2 From Hardware to Software</a></li>
        <li><a href="chapter23.html">2.3 How Does Code Become Software ?</a></li>
        <li><a href="chapter24.html">2.4 What Is an Algorithm?</a></li>
        <li><a href="chapter25.html">2.5 The Sprint</a></li>
        <li><a href="chapter26.html">2.6 What's With All These Conferences ?</a></li>
    </ul></div>

    <div class="column middle" style>
    <p1> <h2>What is a computer?</h2>A computer is <strong>"a clock with benefits"</strong>. 
    They all work the same, doing secondgrade math, one step at a time: Tick, take a number 
    and put it in box one.
    Tick, take another number, put it in box two. Tick, operate (an operation
    might be addition or subtraction) on those two numbers and put the
    resulting number in box one. Tick, check if the result is zero, and if it is, go to
    some other box and follow a new set of instructions.<br></br>
    You, using a pen and paper, can do anything a computer can; you just can't
    do those things billions of times per second. And those billions of tiny
    operations add up. They can cause a phone to boop, elevate an elevator, or
    redirect a missile. That raw speed makes it possible to pull off not one but
    multiple sleights of hand, card tricks on top of card tricks. Take a bunch of
    pulses of light reflected from an optical disc, apply some math to unsqueeze
    them, and copy the resulting pile of expanded impulses into some memory
    cells—then read from those cells to paint light on the screen. Millions of
    pulses, 60 times a second. That's how you make the rubes believe they're
    watching a movie.</p1><br></br>
    <p2>Apple has always made computers; Microsoft used to make only software
    (and occasional accessory hardware, such as mice and keyboards), but now
    it's in the hardware business, with Xbox game consoles, Surface tablets, and
    Lumia phones. Facebook assembles its own computers for its massive data
    centers.<br></br>
    So many things are computers, or will be. That includes watches, cameras,
    air conditioners, cash registers, toilets, toys, airplanes, and movie projectors.
    Samsung makes computers that look like TVs, and Tesla makes computers
    with wheels and engines. Some things that aren't yet computers—dental
    floss, flashlights—will fall eventually.</p2><br></br>
    <p3>When you "batch" process a thousand images in Photoshop or sum numbers
    in Excel, you're programming, at least a little. When you use computers too
    much—which is to say a typical amount—they start to change you. I've had
    Photoshop dreams, Visio dreams, spreadsheet dreams, and Web browser
    dreams. The dreamscape becomes fluid and can be sorted and restructured.
    I've had programming dreams where I move text around the screen.<br></br>
    You can make computers do wonderful things, but you need to understand
    their limits. They're not all-powerful, not conscious in the least. They're fast,
    but some parts—the processor, the RAM—are faster than others—like the
    hard drive or the network connection. Making them seem infinite takes a
    great deal of work from a lot of programmers and a lot of marketers.<br></br>
    The turn-of-last-century British artist William Morris once said you can't
    have art without resistance in the materials. The computer and its
    multifarious peripherals are the materials. The code is the art.</p3>
    <p4><h2>2.1 How does a computer work?</h2>
        Consider what happens when you strike a key on your keyboard. <strong>Say a
        lowercase "a."</strong> The keyboard is waiting for you to press a key, or release one;
        it's constantly scanning to see what keys are pressed down. Hitting the key
        sends a scancode.<br></br>
        Just as the keyboard is waiting for a key to be pressed, the computer is
        waiting for a signal from the keyboard. When one comes down the pike, the
        computer interprets it and passes it farther into its own interior. "Here's what
        the keyboard just received—do with this what you will."<br></br>
        It's simple now, right? The computer just goes to some table, figures out that
        the signal corresponds to the letter "a," and puts it on screen. Of course not—
        too easy. Computers are machines. They don't know what a screen or an “a”
        are. To put the “a” on the screen, your computer has to pull the image of the
        "a" out of its memory as part of a font, an "a" made up of lines and circles. It
        has to take these lines and circles and render them in a little box of pixels in
        the part of its memory that manages the screen. So far we have at least three 
        representations of one letter: the signal from the keyboard; the version in
        memory; and the lines-and-circles version sketched on the screen. We
        haven't even considered how to store it, or what happens to the letters to the
        left and the right when you insert an “a” in the middle of a sentence. Or what
        “lines and circles” mean when reduced to binary data. There are surprisingly
        many ways to represent a simple "a." It's amazing any of it works at all.<br></br>
        Coders are people who are willing to work backward to that key press. It
        takes a certain temperament to page through standards documents,
        manuals, and documentation and read things like “data fields are
        transmitted least significant bit first” in the interest of understanding why,
        when you expected “ü,” you keep getting “�.”
    </p4>
    </div>
    <div class="column side" style>
        <br><br><br></br>
        <p id="p5">
            Ford compares computer to <strong><em>"a clock with benefits"</em></strong>.<br></br>
            Computer takes a pair of numbers and perform a calculation, 
            and then performs another calculation by using the output of 
            those two numbers. <br></br>
            Computer takes only one step at a time. It's powerful than us
            because it can do this billions of 
            times within a second. </p><br></br><br>
        <p id="p6"><strong>What Things Are Computers?</strong><br>
            The production of computers is a huge part of many technology companies' business.<br></br>
            Ford wants us to know that many others things are also computers: many things we use today like washing machine, watches, TV, elevator, etc.</p><br><br><br>
        <p id="p7">
            What I understand here is: people are unconsciously programming when using softwares. 
            Computers are not omnipotent. Coding is collaborative, and requires aesthetics.</p><br></br><br></br><br><br>
        <br></br><br></br><br><br><br><div id="p8">
            There's many <b>steps for computers to go through</b> to execute eg. an "a ":
          <ol>
            <li>Signal(scancode) from keyboard</li>
            <li>Lines-and-circles version in memory</li>
            <li>Sketch lines and circles on the screen</li>
          </ol>
            A coder troubleshoots by understanding the rules governing the sequencing of these steps.
        </div><br><br><br>
        <p id="p9"><b>Computer's not as smart</b> as we think. However, it's amazing that people can direct it to a specific type of information simply by sending a code from the keyboard!</p>
    </div>
</div>
<div class="footer">
    <p1>What is Code? —— Paul Ford</p1>
</div>
</div>
</body>
</html>
